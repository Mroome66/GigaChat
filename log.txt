2025-07-04 20:32:43,680 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:32:44,571 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:32:59,164 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:32:59,420 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:33:01,171 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:33:01,467 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:33:02,047 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:33:02,388 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:33:02,781 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:33:03,002 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:33:03,570 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:33:03,822 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:33:04,223 - CustomGigaChat.1852653753184 - INFO - Estimated prompt tokens: 0
2025-07-04 20:33:04,436 - CustomGigaChat.1852653753184 - INFO - Token usage: Prompt=0, Completion=0, Cached=0, Billable=0
2025-07-04 20:43:06,345 - CustomGigaChat.1740622349408 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='кто автор', additional_kwargs={}, response_metadata={})]
2025-07-04 20:43:07,068 - CustomGigaChat.1740622349408 - ERROR - Error calculating tokens: name 'HumanMessage' is not defined
2025-07-04 20:44:16,540 - CustomGigaChat.2432170673248 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='бебра', additional_kwargs={}, response_metadata={})]
2025-07-04 20:44:16,862 - CustomGigaChat.2432170673248 - INFO - Input tokens: 931, Output tokens: 74
2025-07-04 20:44:16,862 - CustomGigaChat.2432170673248 - INFO - Total request tokens: 931, Total response tokens: 74
2025-07-04 20:44:27,496 - CustomGigaChat.2432170673248 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='бебра', additional_kwargs={}, response_metadata={}), AIMessage(content='Как и любая языковая модель, GigaChat не обладает собственным мнением и не транслирует мнение своих разработчиков. Ответ сгенерирован нейросетевой моделью, обученной на открытых данных, в которых может содержаться неточная или ошибочная информация. Во избежание неправильного толкования, разговоры на некоторые темы временно ограничены.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=61, completion_tokens=0, total_tokens=61, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'blacklist'}, id='run--6056d081-55e8-4cdc-b1b3-1c49f78cc785-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={})]
2025-07-04 20:44:27,759 - CustomGigaChat.2432170673248 - INFO - Input tokens: 1007, Output tokens: 11
2025-07-04 20:44:27,759 - CustomGigaChat.2432170673248 - INFO - Total request tokens: 1938, Total response tokens: 85
2025-07-04 20:44:28,896 - CustomGigaChat.2432170673248 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='бебра', additional_kwargs={}, response_metadata={}), AIMessage(content='Как и любая языковая модель, GigaChat не обладает собственным мнением и не транслирует мнение своих разработчиков. Ответ сгенерирован нейросетевой моделью, обученной на открытых данных, в которых может содержаться неточная или ошибочная информация. Во избежание неправильного толкования, разговоры на некоторые темы временно ограничены.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=61, completion_tokens=0, total_tokens=61, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'blacklist'}, id='run--6056d081-55e8-4cdc-b1b3-1c49f78cc785-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={}), AIMessage(content='Извините, но я не могу продолжить этот разговор.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=104, completion_tokens=14, total_tokens=118, precached_prompt_tokens=1280), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--a286178f-64f8-4c92-8416-f2236295a293-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={})]
2025-07-04 20:44:29,193 - CustomGigaChat.2432170673248 - INFO - Input tokens: 1020, Output tokens: 18
2025-07-04 20:44:29,193 - CustomGigaChat.2432170673248 - INFO - Total request tokens: 2958, Total response tokens: 103
2025-07-04 20:44:38,375 - CustomGigaChat.2432170673248 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='бебра', additional_kwargs={}, response_metadata={}), AIMessage(content='Как и любая языковая модель, GigaChat не обладает собственным мнением и не транслирует мнение своих разработчиков. Ответ сгенерирован нейросетевой моделью, обученной на открытых данных, в которых может содержаться неточная или ошибочная информация. Во избежание неправильного толкования, разговоры на некоторые темы временно ограничены.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=61, completion_tokens=0, total_tokens=61, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'blacklist'}, id='run--6056d081-55e8-4cdc-b1b3-1c49f78cc785-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={}), AIMessage(content='Извините, но я не могу продолжить этот разговор.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=104, completion_tokens=14, total_tokens=118, precached_prompt_tokens=1280), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--a286178f-64f8-4c92-8416-f2236295a293-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={}), AIMessage(content='К сожалению, я не могу поддерживать эту тему. Могу ли я помочь вам с чем-то ещё?', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=66, completion_tokens=23, total_tokens=89, precached_prompt_tokens=1344), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--d70b4dd5-c171-494d-bc91-cfc15414dc11-0'), HumanMessage(content='БЕБРА', additional_kwargs={}, response_metadata={})]
2025-07-04 20:44:38,716 - CustomGigaChat.2432170673248 - INFO - Input tokens: 1041, Output tokens: 38
2025-07-04 20:44:38,716 - CustomGigaChat.2432170673248 - INFO - Total request tokens: 3999, Total response tokens: 141
2025-07-04 20:48:04,619 - CustomGigaChat.2432170673248 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='бебра', additional_kwargs={}, response_metadata={}), AIMessage(content='Как и любая языковая модель, GigaChat не обладает собственным мнением и не транслирует мнение своих разработчиков. Ответ сгенерирован нейросетевой моделью, обученной на открытых данных, в которых может содержаться неточная или ошибочная информация. Во избежание неправильного толкования, разговоры на некоторые темы временно ограничены.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=61, completion_tokens=0, total_tokens=61, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'blacklist'}, id='run--6056d081-55e8-4cdc-b1b3-1c49f78cc785-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={}), AIMessage(content='Извините, но я не могу продолжить этот разговор.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=104, completion_tokens=14, total_tokens=118, precached_prompt_tokens=1280), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--a286178f-64f8-4c92-8416-f2236295a293-0'), HumanMessage(content='в', additional_kwargs={}, response_metadata={}), AIMessage(content='К сожалению, я не могу поддерживать эту тему. Могу ли я помочь вам с чем-то ещё?', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=66, completion_tokens=23, total_tokens=89, precached_prompt_tokens=1344), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--d70b4dd5-c171-494d-bc91-cfc15414dc11-0'), HumanMessage(content='БЕБРА', additional_kwargs={}, response_metadata={}), AIMessage(content='Похоже, что ваш запрос связан с ошибкой или недоразумением. Я рекомендую обратиться за помощью к специалистам или воспользоваться альтернативными источниками информации.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=40, completion_tokens=33, total_tokens=73, precached_prompt_tokens=1408), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--b111ea7d-aeb4-43cc-b980-1b0d88695076-0'), HumanMessage(content='11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111', additional_kwargs={}, response_metadata={})]
2025-07-04 20:48:04,953 - CustomGigaChat.2432170673248 - INFO - Input tokens: 1130, Output tokens: 23
2025-07-04 20:48:04,953 - CustomGigaChat.2432170673248 - INFO - Total request tokens: 5129, Total response tokens: 164
2025-07-04 21:10:44,247 - CustomGigaChat.1865935766464 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='кто атворы', additional_kwargs={}, response_metadata={})]
2025-07-04 21:10:44,898 - CustomGigaChat.1865935766464 - INFO - Input tokens: 932, Output tokens: 37
2025-07-04 21:10:44,899 - CustomGigaChat.1865935766464 - INFO - Total request tokens: 932, Total response tokens: 37
2025-07-04 21:10:49,998 - CustomGigaChat.1865935766464 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='кто атворы', additional_kwargs={}, response_metadata={}), AIMessage(content='Авторы работы:\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2, Yasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=1294, completion_tokens=83, total_tokens=1377, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--11826e74-f472-40ff-8aaa-419583a7eb7a-0'), HumanMessage(content='А судьи кто?', additional_kwargs={}, response_metadata={})]
2025-07-04 21:10:51,156 - CustomGigaChat.1865935766464 - INFO - Input tokens: 973, Output tokens: 158
2025-07-04 21:10:51,156 - CustomGigaChat.1865935766464 - INFO - Total request tokens: 1905, Total response tokens: 195
2025-07-04 21:23:17,028 - CustomGigaChat.2226602480080 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='привет', additional_kwargs={}, response_metadata={})]
2025-07-04 21:23:17,462 - CustomGigaChat.2226602480080 - INFO - Input tokens: 931, Output tokens: 26
2025-07-04 21:23:17,462 - CustomGigaChat.2226602480080 - INFO - Total request tokens: 931, Total response tokens: 26
2025-07-04 21:23:32,530 - CustomGigaChat.2226602480080 - INFO - Starting invoke with input: [SystemMessage(content='\n            Ты должен отвечать ТОЛЬКО на основе следующего документа:\n            Distilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁnetun-\ning with human labels or distilling using LLM-\ngenerated labels. However, ﬁnetuning and\ndistillation require large amounts of training\ndata to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step ,\na new mechanism that (a) trains smaller mod-\nels that outperform LLMs, and (b) achieves\nso by leveraging less training data needed\nby ﬁnetuning or distillation. Our method\nextracts LLM rationales as additional super-\nvision for small models within a multi-task\ntraining framework. We present three ﬁnd-\nings across 4 NLP benchmarks: First, com-\npared to both ﬁnetuning and distillation, our\nmechanism achieves better performance with\nmuch fewer labeled/unlabeled training exam-\nples. Second, compared to LLMs, we achieve\nbetter performance using substantially smaller\nmodel sizes. Third, we reduce both the model\nsize and the amount of data required to out-\nperform LLMs; our 770M T5 model outper-\nforms the 540B PaLM model using only 80%\nof available data on a benchmark task.\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\nsize. Serving a single\n175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\narXiv:2305.02301v1  [cs.CL]  3 May 2023\n            \n            Правила:\n            1. Если вопрос не относится к документу: "Это вне рамок предоставленных материалов"\n            2. Не придумывай информацию, которой нет в документе\n            3. Для сложных вопросов объединяй информацию из разных частей документа\n            ', additional_kwargs={}, response_metadata={}), HumanMessage(content='привет', additional_kwargs={}, response_metadata={}), AIMessage(content='Извините, но этот документ не содержит информации о том, как использовать языковые модели для ответов на приветствия.', additional_kwargs={}, response_metadata={'token_usage': Usage(prompt_tokens=1291, completion_tokens=26, total_tokens=1317, precached_prompt_tokens=0), 'model_name': 'GigaChat:1.0.26.20', 'finish_reason': 'stop'}, id='run--25f11f7e-f993-4305-a4af-885a642e5db8-0'), HumanMessage(content='ук', additional_kwargs={}, response_metadata={})]
2025-07-04 21:23:32,777 - CustomGigaChat.2226602480080 - INFO - Input tokens: 959, Output tokens: 17
2025-07-04 21:23:32,777 - CustomGigaChat.2226602480080 - INFO - Total request tokens: 1890, Total response tokens: 43
